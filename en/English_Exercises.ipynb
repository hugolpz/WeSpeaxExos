{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stanza'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mh:\\Bachelor\\Year-04\\Semester-01\\Project-Matthieu\\WeSpeaxExos\\en\\English_Exercises.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Bachelor/Year-04/Semester-01/Project-Matthieu/WeSpeaxExos/en/English_Exercises.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Bachelor/Year-04/Semester-01/Project-Matthieu/WeSpeaxExos/en/English_Exercises.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/h%3A/Bachelor/Year-04/Semester-01/Project-Matthieu/WeSpeaxExos/en/English_Exercises.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstanza\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Bachelor/Year-04/Semester-01/Project-Matthieu/WeSpeaxExos/en/English_Exercises.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlemminflect\u001b[39;00m \u001b[39mimport\u001b[39;00m getAllInflections\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Bachelor/Year-04/Semester-01/Project-Matthieu/WeSpeaxExos/en/English_Exercises.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m random\u001b[39m.\u001b[39mseed(\u001b[39m12345\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stanza'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import stanza\n",
    "from lemminflect import getAllInflections\n",
    "\n",
    "random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all 3 sheets of the 700 Sentences dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7000_sents_1 = pd.read_excel(\n",
    "    \"../data/corpus/7000 Sentences Corpus With IDs.xlsx\",\n",
    "    sheet_name=\"3000\",\n",
    "    usecols=[\"ID\", \"French\", \"English\"],\n",
    "    na_values=['NA'],\n",
    ")\n",
    "\n",
    "df_7000_sents_2 = pd.read_excel(\n",
    "    \"../data/corpus/7000 Sentences Corpus With IDs.xlsx\",\n",
    "    sheet_name=\"6000\",\n",
    "    usecols=[\"ID\", \"French\", \"English\"],\n",
    "    na_values=['NA'],\n",
    ")\n",
    "\n",
    "df_7000_sents_3 = pd.read_excel(\n",
    "    \"../data/corpus/7000 Sentences Corpus With IDs.xlsx\",\n",
    "    sheet_name=\"1000\",\n",
    "    usecols=[\"ID\", \"French\", \"English\"],\n",
    "    na_values=['NA'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7000_sents_1 = df_7000_sents_1.dropna().reset_index(drop = True)\n",
    "df_7000_sents_2 = df_7000_sents_2.dropna().reset_index(drop = True)\n",
    "df_7000_sents_3 = df_7000_sents_3.dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7000_sents_1 = df_7000_sents_1.set_index([\"ID\", \"French\"]).apply(lambda x: x.str.split('/').explode()).reset_index()\n",
    "df_7000_sents_2 = df_7000_sents_2.set_index([\"ID\", \"French\"]).apply(lambda x: x.str.split('/').explode()).reset_index()\n",
    "df_7000_sents_3 = df_7000_sents_3.set_index([\"ID\", \"French\"]).apply(lambda x: x.str.split('/').explode()).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the 3000 Words dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3000_words = pd.read_excel(\n",
    "    \"../data/corpus/3000 Hindi Words Corpus With IDs.xlsx\",\n",
    "    sheet_name=\"Feuille1\",\n",
    "    usecols=[\"ID\", \"French Word\", \"English Word\"],\n",
    "    na_values=[\"NA\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3000_words = df_3000_words.dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3000_words = df_3000_words.set_index([\"ID\", \"French Word\"]).apply(lambda x: x.str.split('/').explode()).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download a Stanza Language Model for English and French into the directory \"../stanza_models\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download(lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize a Stanza pipeline with a language model for English and French\n",
    "\n",
    "## Which is assigned to the variable 'nlp_en' and 'nlp_fr' using the Pipeline() class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en_stanza = stanza.Pipeline(lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Exercise Dataset Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"Exo_type_id\",\n",
    "    \"Exo_type\",\n",
    "    \"Exo_objective\",\n",
    "    \"Exo_focus\",\n",
    "    \"Exo_id\",\n",
    "    \"Source_format\",\n",
    "    \"Target_format\",\n",
    "    \"Source_sentence_id\",\n",
    "    \"Source_word_id\",\n",
    "    \"Source_lang\",\n",
    "    \"Target_lang\",\n",
    "    \"Full_sentence\",\n",
    "    \"Instruction\",\n",
    "    \"Sentence_w_blank\",\n",
    "    \"Right_answer\",\n",
    "    \"Dist_1\",\n",
    "    \"Dist_2\",\n",
    "    \"Dist_3\",\n",
    "    \"Propositions\",\n",
    "    \"Right_answer_id\",\n",
    "    \"Explanation\",\n",
    "    \"Difficulty\",\n",
    "    \"Remediation\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distractor Creation Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spelling Distractors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exchanges two letters in a given word\n",
    "def replace_letter(a, b, text, list_of_distractors):\n",
    "    if (a in text):\n",
    "        list_of_distractors.append(text.replace(a, b))\n",
    "\n",
    "    if (b in text):\n",
    "        list_of_distractors.append(text.replace(b, a))\n",
    "\n",
    "    return list_of_distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distractor_generator(word, list_of_distractors):\n",
    "    if \"ie\" in word.text:\n",
    "        list_of_distractors = replace_letter(\n",
    "            \"ie\", \"ei\", word.text, list_of_distractors)\n",
    "\n",
    "    if \"ei\" in word.text:\n",
    "        list_of_distractors = replace_letter(\n",
    "            \"ei\", \"ie\", word.text, list_of_distractors)\n",
    "\n",
    "    consonats = [\n",
    "        'b', 'c', 'd', 'f',\n",
    "        'g', 'h', 'j', 'k',\n",
    "        'l', 'm', 'n', 'p',\n",
    "        'q', 'r', 's', 't',\n",
    "        'v', 'w', 'x', 'y',\n",
    "        'z'\n",
    "    ]\n",
    "    for i in consonats:\n",
    "        txt = i + i\n",
    "        if txt in word.text:\n",
    "            list_of_distractors = replace_letter(\n",
    "                txt, i, word.text, list_of_distractors)\n",
    "\n",
    "    if 'ant' in word.text:\n",
    "        list_of_distractors = replace_letter(\n",
    "            'ant', 'ent', word.text, list_of_distractors)\n",
    "\n",
    "    if 'ent' in word.text:\n",
    "        list_of_distractors = replace_letter(\n",
    "            'ent', 'ant', word.text, list_of_distractors)\n",
    "\n",
    "    if 'ance' in word.text:\n",
    "        list_of_distractors = replace_letter(\n",
    "            'ance', 'ence', word.text, list_of_distractors)\n",
    "\n",
    "    if 'ence' in word.text:\n",
    "        list_of_distractors = replace_letter(\n",
    "            'ence', 'ance', word.text, list_of_distractors)\n",
    "\n",
    "    if 'ar' in word.text:\n",
    "        list_of_distractors = replace_letter(\n",
    "            'ar', 'er', word.text, list_of_distractors)\n",
    "\n",
    "    if 'er' in word.text:\n",
    "        list_of_distractors = replace_letter(\n",
    "            'er', 'ar', word.text, list_of_distractors)\n",
    "\n",
    "    if 'ary' in word.text:\n",
    "        list_of_distractors = replace_letter(\n",
    "            'ary', 'ery', word.text, list_of_distractors)\n",
    "\n",
    "    if 'ery' in word.text:\n",
    "        list_of_distractors = replace_letter(\n",
    "            'ery', 'ary', word.text, list_of_distractors)\n",
    "\n",
    "    if 'er' in word.text:\n",
    "        list_of_distractors = replace_letter(\n",
    "            'er', 'eur', word.text, list_of_distractors)\n",
    "\n",
    "    if 'eur' in word.text:\n",
    "        list_of_distractors = replace_letter(\n",
    "            'eur', 'er', word.text, list_of_distractors)\n",
    "\n",
    "    if 'ea' in word.text:\n",
    "        list_of_distractors = replace_letter(\n",
    "            'ea', 'e', word.text, list_of_distractors)\n",
    "\n",
    "    if 'ly' in word.text:\n",
    "        list_of_distractors = replace_letter(\n",
    "            'ly', 'ely', word.text, list_of_distractors)\n",
    "\n",
    "    if 'ely' in word.text:\n",
    "        list_of_distractors = replace_letter(\n",
    "            'ely', 'ly', word.text, list_of_distractors)\n",
    "\n",
    "    if 'ies' in word.text:\n",
    "        list_of_distractors = replace_letter(\n",
    "            'ies', 'ys', word.text, list_of_distractors)\n",
    "\n",
    "    if 'ys' in word.text:\n",
    "        list_of_distractors = replace_letter(\n",
    "            'ys', 'ies', word.text, list_of_distractors)\n",
    "\n",
    "    return list_of_distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 spelling distractors for a given word\n",
    "def spelling_distractors(word, list_of_distractors=None):\n",
    "\n",
    "    if list_of_distractors is None:\n",
    "        list_of_distractors = []\n",
    "        \n",
    "    list_of_distractors = distractor_generator(word, list_of_distractors)\n",
    "\n",
    "    if len(list_of_distractors) >= 3:\n",
    "        list_of_distractors = list_of_distractors[:3]\n",
    "    else:\n",
    "        list_of_distractors = []\n",
    "\n",
    "    return list_of_distractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Verb Distractors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_distractors(word, pos, list_of_distractors=None):\n",
    "\n",
    "    if list_of_distractors is None:\n",
    "        list_of_distractors = []\n",
    "        \n",
    "    list_of_distractors = set(sum(getAllInflections(word, upos=pos).values(),()))\n",
    "    list_of_distractors.discard(word)\n",
    "    if len(list_of_distractors) >= 3:\n",
    "        list_of_distractors = list(random.sample(list(list_of_distractors), 3))\n",
    "    else:\n",
    "        list_of_distractors = []\n",
    "    return list_of_distractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Vocabulary Flashcards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_flashcards(df_source):\n",
    "    data = []\n",
    "    exo_id = 0\n",
    "    for i in range(len(df_source)):\n",
    "        if df_source[\"English Word\"][i] != \"NaN\":\n",
    "            exo_id += 1\n",
    "            data.append([\n",
    "                \"10\",\n",
    "                \"Flashcards\",\n",
    "                \"Learning_Vocabulary\",\n",
    "                \"\",\n",
    "                str(exo_id),\n",
    "                \"text\",\n",
    "                \"text\",\n",
    "                \"\",\n",
    "                str(df_source[\"ID\"][i]),\n",
    "                \"French\",\n",
    "                \"English\",\n",
    "                str(df_source[\"English Word\"][i]),\n",
    "                \"\",\n",
    "                \"\",\n",
    "                str(df_source[\"French Word\"][i]),\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                str(df_source[\"French Word\"][i]),\n",
    "                \"0\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "            ])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w_fc_1 = word_flashcards(df_3000_words)\n",
    "df_w_fc_1 = pd.DataFrame(data_w_fc_1, columns=cols)\n",
    "\n",
    "frames_w_fc = [df_w_fc_1]\n",
    "df_w_fc = pd.concat(frames_w_fc)\n",
    "df_w_fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spelling MCQ Exercises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_mcq(df_source, exo_id, cols):\n",
    "    data = []\n",
    "\n",
    "    spellings = {col: [] for col in cols}\n",
    "\n",
    "    for i in range(len(df_source)):\n",
    "        doc_en_phrase = nlp_en_stanza(str(df_source[\"English Word\"][i]))\n",
    "\n",
    "        for sent in doc_en_phrase.sentences:\n",
    "            for word in sent.words:\n",
    "                list_of_distractors = []\n",
    "\n",
    "                if word.text not in spellings[\"Right_answer\"]:\n",
    "                    list_of_distractors = spelling_distractors(word, list_of_distractors)\n",
    "\n",
    "                    if list_of_distractors != []:\n",
    "                        list_of_distractors += [word.text]\n",
    "                        random.shuffle(list_of_distractors)\n",
    "                        right_answer_id = list_of_distractors.index(word.text)\n",
    "                        options = \"\".join(f\"-{dist}\" for dist in list_of_distractors)\n",
    "                        \n",
    "                        exo_id += 1\n",
    "\n",
    "                        spellings[\"Exo_type_id\"].append(\"14\")\n",
    "                        spellings[\"Exo_type\"].append(\"MCQ\")\n",
    "                        spellings[\"Exo_objective\"].append(\"Learning vocabulary\")\n",
    "                        spellings[\"Exo_focus\"].append(\"Spellings\")\n",
    "                        spellings[\"Exo_id\"].append(str(exo_id))\n",
    "                        spellings[\"Source_format\"].append(\"text\")\n",
    "                        spellings[\"Target_format\"].append(\"text\")\n",
    "                        spellings[\"Source_sentence_id\"].append(\"\")\n",
    "                        spellings[\"Source_word_id\"].append(str(df_source[\"ID\"][i]))\n",
    "                        spellings[\"Source_lang\"].append(\"French\")\n",
    "                        spellings[\"Target_lang\"].append(\"English\")\n",
    "                        spellings[\"Full_sentence\"].append(str(word.text))\n",
    "                        spellings[\"Instruction\"].append(\"Select the correct spelling (for \" + str(df_source[\"French Word\"][i]) + \"): \")\n",
    "                        spellings[\"Sentence_w_blank\"].append(\"\")\n",
    "                        spellings[\"Right_answer\"].append(str(word.text))\n",
    "                        spellings[\"Dist_1\"].append(list_of_distractors[0]),\n",
    "                        spellings[\"Dist_2\"].append(list_of_distractors[1]),\n",
    "                        spellings[\"Dist_3\"].append(list_of_distractors[2]),\n",
    "                        spellings[\"Propositions\"].append(options),\n",
    "                        spellings[\"Right_answer_id\"].append(str(right_answer_id)),\n",
    "                        spellings[\"Explanation\"].append(\"\")\n",
    "                        spellings[\"Difficulty\"].append(\"\")\n",
    "                        spellings[\"Remediation\"].append(\"\")\n",
    "\n",
    "    return exo_id, spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo_id = 0\n",
    "exo_id, data_spelling_mcq_1 = spelling_mcq(df_3000_words, exo_id, cols)\n",
    "\n",
    "df_spelling_mcq_1 = pd.DataFrame(data_spelling_mcq_1, columns=cols)\n",
    "\n",
    "frames_spelling_mcq = [df_spelling_mcq_1]\n",
    "df_spelling_mcq = pd.concat(frames_spelling_mcq)\n",
    "df_spelling_mcq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Useful Sentences Flashcards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sentence_flashcards(df_source):\n",
    "    return [\n",
    "        [\n",
    "            \"24\", \n",
    "            \"Flashcards\", \n",
    "            \"Useful_Sentences\", \n",
    "            \"\", \n",
    "            str(exo_id), \n",
    "            \"text\", \n",
    "            \"text\", \n",
    "            str(df_source[\"ID\"][i]), \n",
    "            \"\",\n",
    "            \"French\", \n",
    "            \"English\", \n",
    "            str(df_source[\"English\"][i]), \n",
    "            \"\", \n",
    "            \"\", \n",
    "            str(df_source[\"French\"][i]), \n",
    "            \"\", \n",
    "            \"\", \n",
    "            \"\", \n",
    "            str(df_source[\"French\"][i]),\n",
    "            \"0\", \n",
    "            \"\", \n",
    "            \"\", \n",
    "            \"\",\n",
    "        ] \n",
    "        for exo_id, i in enumerate(range(len(df_source)))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s_fc = sentence_flashcards(df_7000_sents_3)\n",
    "df_s_fc = pd.DataFrame(data_s_fc, columns=cols)\n",
    "\n",
    "df_s_fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Verb Conjugation MCQ Exercises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_conjug_mcq(df_source, exo_id, cols):\n",
    "\n",
    "    verb_conjug = {col: [] for col in cols}\n",
    "\n",
    "    for i in range(len(df_source)):\n",
    "        doc_en_phrase = nlp_en_stanza(str(df_source[\"English\"][i]))\n",
    "\n",
    "        count = 0\n",
    "        blank = []\n",
    "        answer = []\n",
    "        hint = []\n",
    "        for sent in doc_en_phrase.sentences:\n",
    "            for word in sent.words:\n",
    "                if word.upos in [\"VERB\", \"AUX\"] and word.text != word.lemma:\n",
    "                    blank.append(count)\n",
    "                    answer.append(word.text)\n",
    "                    hint.append(word.lemma)\n",
    "\n",
    "                count += 1\n",
    "\n",
    "        if len(blank):\n",
    "            question = []\n",
    "\n",
    "            for b in range(len(blank)):\n",
    "\n",
    "                question.append(\n",
    "                    \" \".join(                              \n",
    "                        [\n",
    "                            word.text if word.id != blank[b] + 1 else \"...\" for word in sent.words for sent in doc_en_phrase.sentences\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                list_of_distractors = []\n",
    "                list_of_distractors = verb_distractors(answer[b], nlp_en_stanza(answer[b]).sentences[0].words[0].upos)\n",
    "\n",
    "                if list_of_distractors != []:\n",
    "                    list_of_distractors += [word.text]\n",
    "                    random.shuffle(list_of_distractors)\n",
    "                    right_answer_id = list_of_distractors.index(word.text)\n",
    "                    options = \"\".join(f\"-{dist}\" for dist in list_of_distractors)\n",
    "                    \n",
    "                    exo_id += 1\n",
    "\n",
    "                    verb_conjug[\"Exo_type_id\"].append(\"35\")\n",
    "                    verb_conjug[\"Exo_type\"].append(\"MCQ\")\n",
    "                    verb_conjug[\"Exo_objective\"].append(\"Grammar\")\n",
    "                    verb_conjug[\"Exo_focus\"].append(\"Verb_Conjugation\")\n",
    "                    verb_conjug[\"Exo_id\"].append(str(exo_id))\n",
    "                    verb_conjug[\"Source_format\"].append(\"text\")\n",
    "                    verb_conjug[\"Target_format\"].append(\"text\")\n",
    "                    verb_conjug[\"Source_sentence_id\"].append(str(df_source[\"ID\"][i]))\n",
    "                    verb_conjug[\"Source_word_id\"].append(\"\")\n",
    "                    verb_conjug[\"Source_lang\"].append(\"French\")\n",
    "                    verb_conjug[\"Target_lang\"].append(\"English\")\n",
    "                    verb_conjug[\"Full_sentence\"].append(str(df_source[\"English\"][i]))\n",
    "                    verb_conjug[\"Instruction\"].append(\"Conjugate the verb correctly:\")\n",
    "                    verb_conjug[\"Sentence_w_blank\"].append(str(f\"{question[b]} ({hint[b]})\"))\n",
    "                    verb_conjug[\"Right_answer\"].append(str(answer[b]))\n",
    "                    verb_conjug[\"Dist_1\"].append(list_of_distractors[0]),\n",
    "                    verb_conjug[\"Dist_2\"].append(list_of_distractors[1]),\n",
    "                    verb_conjug[\"Dist_3\"].append(list_of_distractors[2]),\n",
    "                    verb_conjug[\"Propositions\"].append(options),\n",
    "                    verb_conjug[\"Right_answer_id\"].append(str(right_answer_id)),\n",
    "                    verb_conjug[\"Explanation\"].append(\"\")\n",
    "                    verb_conjug[\"Difficulty\"].append(\"\")\n",
    "                    verb_conjug[\"Remediation\"].append(\"\")\n",
    "\n",
    "    return exo_id, verb_conjug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo_id = 0\n",
    "\n",
    "exo_id, data_v_conjug_mcq_1 = verb_conjug_mcq(df_7000_sents_1, exo_id, cols)\n",
    "df_v_conjug_mcq_1 = pd.DataFrame(data_v_conjug_mcq_1, columns=cols)\n",
    "\n",
    "exo_id, data_v_conjug_mcq_2 = verb_conjug_mcq(df_7000_sents_2, exo_id, cols)\n",
    "df_v_conjug_mcq_2 = pd.DataFrame(data_v_conjug_mcq_2, columns=cols)\n",
    "\n",
    "exo_id, data_v_conjug_mcq_3 = verb_conjug_mcq(df_7000_sents_3, exo_id, cols)\n",
    "df_v_conjug_mcq_3 = pd.DataFrame(data_v_conjug_mcq_3, columns=cols)\n",
    "\n",
    "frames_v_conjug_mcq = [df_v_conjug_mcq_1, df_v_conjug_mcq_2, df_v_conjug_mcq_3]\n",
    "df_v_conjug_mcq = pd.concat(frames_v_conjug_mcq)\n",
    "df_v_conjug_mcq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Verb Conjugation Cloze Test Exercises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbs_cloze_test(df_source):\n",
    "\n",
    "    data = []\n",
    "    exo_id = 0\n",
    "    for i in range(len(df_source)):\n",
    "        doc_en_phrase = nlp_en_stanza(str(df_source[\"English\"][i]))\n",
    "\n",
    "        count = 0\n",
    "        blank = []\n",
    "        answer = []\n",
    "        hint = []\n",
    "        for sent in doc_en_phrase.sentences:\n",
    "            for word in sent.words:\n",
    "                if word.upos in [\"VERB\", \"AUX\"] and word.text != word.lemma:\n",
    "                    blank.append(count)\n",
    "                    answer.append(word.text)\n",
    "                    hint.append(word.lemma)\n",
    "\n",
    "                count += 1\n",
    "\n",
    "        if len(blank):\n",
    "            question = []\n",
    "\n",
    "            for b in range(len(blank)):\n",
    "                exo_id += 1\n",
    "                question.append(\" \".join(\n",
    "                    [word.text if word.id != blank[b] + 1 else \"...\" for word in sent.words for sent in doc_en_phrase.sentences]))\n",
    "                data.append([\n",
    "                    \"38\", \n",
    "                    \"Cloze_Test\", \n",
    "                    \"Grammar\", \n",
    "                    \"Verb_Conjugation\", \n",
    "                    str(exo_id), \n",
    "                    \"text\", \n",
    "                    \"text\", \n",
    "                    str(df_source[\"ID\"][i]), \n",
    "                    \"\", \n",
    "                    \"French\", \n",
    "                    \"English\", \n",
    "                    str(df_source[\"English\"][i]), \n",
    "                    \"Conjugate the verb correctly:\", \n",
    "                    f\"{question[b]} ({hint[b]})\", \n",
    "                    str(answer[b]), \n",
    "                    \"\", \n",
    "                    \"\", \n",
    "                    \"\", \n",
    "                    str(answer[b]), \n",
    "                    \"0\", \n",
    "                    \"\",\n",
    "                    \"\",\n",
    "                    \"\",\n",
    "                ])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_v_ct_1 = verbs_cloze_test(df_7000_sents_1)\n",
    "df_v_ct_1 = pd.DataFrame(data_v_ct_1, columns=cols)\n",
    "\n",
    "data_v_ct_2 = verbs_cloze_test(df_7000_sents_2)\n",
    "df_v_ct_2 = pd.DataFrame(data_v_ct_2, columns=cols)\n",
    "\n",
    "data_v_ct_3 = verbs_cloze_test(df_7000_sents_3)\n",
    "df_v_ct_3 = pd.DataFrame(data_v_ct_3, columns=cols)\n",
    "\n",
    "frames_v_ct = [df_v_ct_1, df_v_ct_2, df_v_ct_3]\n",
    "df_v_ct = pd.concat(frames_v_ct)\n",
    "df_v_ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge All Exercise Dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_en_exercises = [df_w_fc, df_spelling_mcq, df_s_fc, df_v_conjug_mcq, df_v_ct]\n",
    "df_en_exercises = pd.concat(frames_en_exercises)\n",
    "df_en_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Exercise Dataframe to an Excel File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_exercises.to_excel(\"English_Exercises.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('intern_2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c272685a0b45085a52c111f1828a3169c5ab3d57d53106d9c21f6d5e8ee41c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
