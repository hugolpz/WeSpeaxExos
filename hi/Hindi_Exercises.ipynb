{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all sheets from the 7000 Sentences dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7000_sents_1 = pd.read_excel(\n",
    "    \"../data/corpus/7000 Sentences Corpus With IDs.xlsx\",\n",
    "    sheet_name=\"3000\",\n",
    "    usecols=[\"ID\", \"French\", \"Hindi\"],\n",
    "    na_values=['NA'],\n",
    ")\n",
    "\n",
    "df_7000_sents_2 = pd.read_excel(\n",
    "    \"../data/corpus/7000 Sentences Corpus With IDs.xlsx\",\n",
    "    sheet_name=\"6000\",\n",
    "    usecols=[\"ID\", \"French\", \"Hindi\"],\n",
    "    na_values=['NA'],\n",
    ")\n",
    "\n",
    "df_7000_sents_3 = pd.read_excel(\n",
    "    \"../data/corpus/7000 Sentences Corpus With IDs.xlsx\",\n",
    "    sheet_name=\"1000\",\n",
    "    usecols=[\"ID\", \"French\", \"Hindi\"],\n",
    "    na_values=['NA'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7000_sents_1 = df_7000_sents_1.dropna().reset_index(drop = True)\n",
    "df_7000_sents_2 = df_7000_sents_2.dropna().reset_index(drop = True)\n",
    "df_7000_sents_3 = df_7000_sents_3.dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the 3000 Words dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3000_words = pd.read_excel(\n",
    "    \"../data/corpus/3000 Hindi Words Corpus With IDs.xlsx\",\n",
    "    sheet_name=\"Feuille1\",\n",
    "    usecols=[\"ID\", \"French Word\", \"Hindi Word\"],\n",
    "    na_values=[\"NA\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3000_words = df_3000_words.dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download a Stanza Language Model for Hindi and French into the directory \"../stanza_models\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download(lang='hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize a Stanza pipeline with a language model for Hindi and French\n",
    "\n",
    "## Which is assigned to the variable 'nlp_hi' and 'nlp_fr' using the Pipeline() class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_hi_stanza = stanza.Pipeline(lang='hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Exercise Dataset Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"Exo_type_id\",\n",
    "    \"Exo_type\",\n",
    "    \"Exo_objective\",\n",
    "    \"Exo_focus\",\n",
    "    \"Exo_id\",\n",
    "    \"Source_format\",\n",
    "    \"Target_format\",\n",
    "    \"Source_sentence_id\",\n",
    "    \"Source_word_id\",\n",
    "    \"Source_lang\",\n",
    "    \"Target_lang\",\n",
    "    \"Full_sentence\",\n",
    "    \"Instruction\",\n",
    "    \"Sentence_w_blank\",\n",
    "    \"Right_answer\",\n",
    "    \"Dist_1\",\n",
    "    \"Dist_2\",\n",
    "    \"Dist_3\",\n",
    "    \"Explanation\",\n",
    "    \"Difficulty\",\n",
    "    \"Remediation\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distractor Creation Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Distractors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exchanges two letters in a given word\n",
    "def exchange_letter(a, b, text, list_of_distractors):\n",
    "    if (a in text):\n",
    "        list_of_distractors.append(text.replace(a, b))\n",
    "\n",
    "    if (b in text):\n",
    "        list_of_distractors.append(text.replace(b, a))\n",
    "\n",
    "    return (list_of_distractors)\n",
    "\n",
    "# Mistakes with similar sounding and looking vowels\n",
    "def vowel_changer(word, list_of_distractors):\n",
    "    if ('इ' in word.text or 'ई' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'इ', 'ई', word.text, list_of_distractors)\n",
    "\n",
    "    if ('ि' in word.text or 'ी' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'ि', 'ी', word.text, list_of_distractors)\n",
    "\n",
    "    if ('उ' in word.text or 'ऊ' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'उ', 'ऊ', word.text, list_of_distractors)\n",
    "\n",
    "    if ('ु' in word.text or 'ू' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'ु', 'ू', word.text, list_of_distractors)\n",
    "\n",
    "    if ('ए' in word.text or 'ऐ' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'ए', 'ऐ', word.text, list_of_distractors)\n",
    "\n",
    "    if ('े' in word.text or 'ै' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'े', 'ै', word.text, list_of_distractors)\n",
    "\n",
    "    if ('ओ' in word.text or 'औ' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'ओ', 'औ', word.text, list_of_distractors)\n",
    "\n",
    "    if ('ो' in word.text or 'ौ' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'ो', 'ौ', word.text, list_of_distractors)\n",
    "\n",
    "    if ('अं' in word.text or 'अँ' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'अं', 'अँ', word.text, list_of_distractors)\n",
    "\n",
    "    if ('ं' in word.text or 'ँ' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'ं', 'ँ', word.text, list_of_distractors)\n",
    "\n",
    "    if ('ॉ' in word.text or 'ाँ' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'ॉ', 'ाँ', word.text, list_of_distractors)\n",
    "\n",
    "    if ('ॉ' in word.text or 'ां' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'ॉ', ' ां', word.text, list_of_distractors)\n",
    "\n",
    "    if ('ाँ' in word.text or 'ां' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'ाँ', 'ां', word.text, list_of_distractors)\n",
    "\n",
    "    return list_of_distractors\n",
    "\n",
    "\n",
    "# Mistakes with similar sounding consonents\n",
    "def consonent_changer(word, list_of_distractors):\n",
    "    if ('ट' in word.text or 'त' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'ट', 'त', word.text, list_of_distractors)\n",
    "\n",
    "    if ('ठ' in word.text or 'थ' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'ठ', 'थ', word.text, list_of_distractors)\n",
    "\n",
    "    if ('ड' in word.text or 'द' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'ड', 'द', word.text, list_of_distractors)\n",
    "\n",
    "    if ('ढ' in word.text or 'ध' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'ढ', 'ध', word.text, list_of_distractors)\n",
    "\n",
    "    if ('न' in word.text or 'ण' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'न', 'ण', word.text, list_of_distractors)\n",
    "\n",
    "    if ('श' in word.text or 'ष' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'श', 'ष', word.text, list_of_distractors)\n",
    "\n",
    "    if ('श' in word.text or 'स' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'श', 'स', word.text, list_of_distractors)\n",
    "\n",
    "    if ('स' in word.text or 'ष' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'स', 'ष', word.text, list_of_distractors)\n",
    "\n",
    "    return list_of_distractors\n",
    "\n",
    "# Mistakes with similar looking letters\n",
    "def letter_changer(word, list_of_distractors):\n",
    "    if ('ज' in word.text or 'ज्ञ' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'ज', 'ज्ञ', word.text, list_of_distractors)\n",
    "\n",
    "    if ('क्ष' in word.text or 'श्र' in word.text):\n",
    "        list_of_distractors = exchange_letter(\n",
    "            'क्ष', 'श्र', word.text, list_of_distractors)\n",
    "\n",
    "    return list_of_distractors\n",
    "\n",
    "# Create 3 spelling distractors for a given word\n",
    "def spelling_distractors(word, list_of_distractors=None):\n",
    "\n",
    "    if list_of_distractors is None:\n",
    "        list_of_distractors = []\n",
    "\n",
    "    list_of_distractors = vowel_changer(word, list_of_distractors)\n",
    "    if len(list_of_distractors) < 3:\n",
    "        list_of_distractors = consonent_changer(word, list_of_distractors)\n",
    "    \n",
    "    if len(list_of_distractors) < 3:\n",
    "        list_of_distractors = letter_changer(word, list_of_distractors)\n",
    "\n",
    "    if len(list_of_distractors) >= 3:\n",
    "        list_of_distractors = list_of_distractors[:3]\n",
    "    else:\n",
    "        list_of_distractors = []\n",
    "\n",
    "    return list_of_distractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Vocabulary Flashcards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_flashcards(df_source, exo_id = 0):\n",
    "    data = []\n",
    "    for i in range(len(df_source)):\n",
    "        exo_id += 1\n",
    "        data.append([\n",
    "            \"10\",\n",
    "            \"Flashcards\",\n",
    "            \"Learning vocabulary\",\n",
    "            \"\",\n",
    "            str(exo_id),\n",
    "            \"text\",\n",
    "            \"text\",\n",
    "            \"\",\n",
    "            str(df_source[\"ID\"][i]),\n",
    "            \"French\",\n",
    "            \"Hindi\",\n",
    "            str(df_source[\"Hindi Word\"][i]),\n",
    "            \"\",\n",
    "            \"\",\n",
    "            str(df_source[\"French Word\"][i]),\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "        ])\n",
    "    return exo_id, data\n",
    "\n",
    "exo_id = 0\n",
    "\n",
    "exo_id, data_w_fc_1 = word_flashcards(df_3000_words, exo_id)\n",
    "df_w_fc_1 = pd.DataFrame(data_w_fc_1, columns=cols)\n",
    "\n",
    "frames_w_fc = [df_w_fc_1]\n",
    "df_w_fc = pd.concat(frames_w_fc)\n",
    "df_w_fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spelling MCQ Exercises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_mcq(df_source, exo_id, cols):\n",
    "    data = []\n",
    "\n",
    "    spellings = {col: [] for col in cols}\n",
    "\n",
    "\n",
    "    for i in range(len(df_source)):\n",
    "        doc_hi_phrase = nlp_hi_stanza(str(df_source[\"Hindi Word\"][i]))\n",
    "\n",
    "        for sent in doc_hi_phrase.sentences:\n",
    "            for word in sent.words:\n",
    "                list_of_distractors = []\n",
    "\n",
    "                if word.text not in spellings[\"Right_answer\"]:\n",
    "                    list_of_distractors = spelling_distractors(word, list_of_distractors)\n",
    "\n",
    "                    if list_of_distractors != []:\n",
    "                        \n",
    "                        exo_id += 1\n",
    "\n",
    "                        spellings[\"Exo_type_id\"].append(\"14\")\n",
    "                        spellings[\"Exo_type\"].append(\"MCQ\")\n",
    "                        spellings[\"Exo_objective\"].append(\"Learning vocabulary\")\n",
    "                        spellings[\"Exo_focus\"].append(\"Spellings\")\n",
    "                        spellings[\"Exo_id\"].append(str(exo_id))\n",
    "                        spellings[\"Source_format\"].append(\"text\")\n",
    "                        spellings[\"Target_format\"].append(\"text\")\n",
    "                        spellings[\"Source_sentence_id\"].append(\"\")\n",
    "                        spellings[\"Source_word_id\"].append(str(df_source[\"ID\"][i]))\n",
    "                        spellings[\"Source_lang\"].append(\"French\")\n",
    "                        spellings[\"Target_lang\"].append(\"Hindi\")\n",
    "                        spellings[\"Full_sentence\"].append(str(df_source[\"Hindi Word\"][i]))\n",
    "                        spellings[\"Instruction\"].append(\"Select the correct spelling:\")\n",
    "                        spellings[\"Sentence_w_blank\"].append(\"\")\n",
    "                        spellings[\"Right_answer\"].append(str(word.text))\n",
    "                        spellings[\"Dist_1\"].append(list_of_distractors[0]),\n",
    "                        spellings[\"Dist_2\"].append(list_of_distractors[1]),\n",
    "                        spellings[\"Dist_3\"].append(list_of_distractors[2]),\n",
    "                        spellings[\"Explanation\"].append(\"\")\n",
    "                        spellings[\"Difficulty\"].append(\"\")\n",
    "                        spellings[\"Remediation\"].append(\"\")\n",
    "\n",
    "    return exo_id, spellings\n",
    "\n",
    "exo_id = 0\n",
    "\n",
    "exo_id, data_spelling_mcq_1 = spelling_mcq(df_3000_words, exo_id, cols)\n",
    "df_spelling_mcq_1 = pd.DataFrame(data_spelling_mcq_1, columns=cols)\n",
    "\n",
    "frames_spelling_mcq = [df_spelling_mcq_1]\n",
    "df_spelling_mcq = pd.concat(frames_spelling_mcq)\n",
    "df_spelling_mcq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Useful Sentences Flashcards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_flashcards(df_source):\n",
    "    data = []\n",
    "    exo_id = 0\n",
    "    for i in range(len(df_source)):\n",
    "        if df_source[\"Hindi\"][i] != \"NaN\":\n",
    "            exo_id += 1\n",
    "            data.append([\n",
    "                \"24\",\n",
    "                \"Flashcards\",\n",
    "                \"Useful Sentences\",\n",
    "                \"\",\n",
    "                str(exo_id),\n",
    "                \"text\",\n",
    "                \"text\",\n",
    "                str(df_source[\"ID\"][i]),\n",
    "                \"\",\n",
    "                \"French\",\n",
    "                \"Hindi\",\n",
    "                str(df_source[\"Hindi\"][i]),\n",
    "                \"\",\n",
    "                \"\",\n",
    "                str(df_source[\"French\"][i]),\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "            ])\n",
    "    return data\n",
    "\n",
    "data_s_fc_3 = sentence_flashcards(df_7000_sents_3)\n",
    "df_s_fc_3 = pd.DataFrame(data_s_fc_3, columns=cols)\n",
    "\n",
    "frames_s_fc = [df_s_fc_3]\n",
    "df_s_fc = pd.concat(frames_s_fc)\n",
    "df_s_fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Verb Conjugation MCQ Exercises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_conjug_mcq(df_source, exo_id, cols):\n",
    "    \n",
    "    verb_conjug = {col: [] for col in cols}\n",
    "\n",
    "    for i in range(len(df_source)):\n",
    "        doc_hi_phrase = nlp_hi_stanza(str(df_source[\"Hindi\"][i]))\n",
    "\n",
    "        count = 0\n",
    "        blank = []\n",
    "        answer = []\n",
    "        hint = []\n",
    "\n",
    "        for sent in doc_hi_phrase.sentences:\n",
    "            for word in sent.words:\n",
    "                if word.upos in [\"VERB\", \"AUX\"] and word.text != word.lemma:\n",
    "                    blank.append(count)\n",
    "                    answer.append(word.text)\n",
    "                    hint.append(word.lemma)\n",
    "\n",
    "                count += 1\n",
    "\n",
    "        if len(blank):\n",
    "            question = []\n",
    "\n",
    "            for b in range(len(blank)):\n",
    "\n",
    "                question.append(\n",
    "                    \" \".join([\n",
    "                        word.text \n",
    "                        if word.id != blank[b] + 1\n",
    "                        else \"...\" \n",
    "                        for word in sent.words \n",
    "                        for sent in doc_hi_phrase.sentences\n",
    "                    ])\n",
    "                )\n",
    "\n",
    "                list_of_distractors = []\n",
    "                list_of_distractors = spelling_distractors(nlp_hi_stanza(answer[b]), list_of_distractors)\n",
    "\n",
    "                if list_of_distractors != []:\n",
    "\n",
    "                    exo_id += 1\n",
    "\n",
    "                    verb_conjug[\"Exo_type_id\"].append(\"35\")\n",
    "                    verb_conjug[\"Exo_type\"].append(\"MCQ\")\n",
    "                    verb_conjug[\"Exo_objective\"].append(\"Grammar\")\n",
    "                    verb_conjug[\"Exo_focus\"].append(\n",
    "                        \"Verb_Conjugation\")\n",
    "                    verb_conjug[\"Exo_id\"].append(str(exo_id))\n",
    "                    verb_conjug[\"Source_format\"].append(\"text\")\n",
    "                    verb_conjug[\"Target_format\"].append(\"text\")\n",
    "                    verb_conjug[\"Source_sentence_id\"].append(\n",
    "                        str(df_source[\"ID\"][i]))\n",
    "                    verb_conjug[\"Source_word_id\"].append(\"\")\n",
    "                    verb_conjug[\"Source_lang\"].append(\"French\")\n",
    "                    verb_conjug[\"Target_lang\"].append(\"Hindi\")\n",
    "                    verb_conjug[\"Full_sentence\"].append(\n",
    "                        str(df_source[\"French\"][i]))\n",
    "                    verb_conjug[\"Instruction\"].append(\n",
    "                        \"Conjugate the verb correctly:\")\n",
    "                    verb_conjug[\"Sentence_w_blank\"].append(str(f\"{question[b]} ({hint[b]})\"))\n",
    "                    verb_conjug[\"Right_answer\"].append(str(answer[b]))\n",
    "                    verb_conjug[\"Dist_1\"].append(list_of_distractors[0]),\n",
    "                    verb_conjug[\"Dist_2\"].append(list_of_distractors[1]),\n",
    "                    verb_conjug[\"Dist_3\"].append(list_of_distractors[2]),\n",
    "                    verb_conjug[\"Explanation\"].append(\"\")\n",
    "                    verb_conjug[\"Difficulty\"].append(\"\")\n",
    "                    verb_conjug[\"Remediation\"].append(\"\")\n",
    "\n",
    "    return exo_id, verb_conjug\n",
    "\n",
    "exo_id = 0\n",
    "\n",
    "exo_id, data_v_conjug_mcq_1 = verb_conjug_mcq(df_7000_sents_1, exo_id, cols)\n",
    "df_v_conjug_mcq_1 = pd.DataFrame(data_v_conjug_mcq_1, columns=cols)\n",
    "\n",
    "exo_id, data_v_conjug_mcq_2 = verb_conjug_mcq(df_7000_sents_2, exo_id, cols)\n",
    "df_v_conjug_mcq_2 = pd.DataFrame(data_v_conjug_mcq_2, columns=cols)\n",
    "\n",
    "exo_id, data_v_conjug_mcq_3 = verb_conjug_mcq(df_7000_sents_3, exo_id, cols)\n",
    "df_v_conjug_mcq_3 = pd.DataFrame(data_v_conjug_mcq_3, columns=cols)\n",
    "\n",
    "frames_v_conjug_mcq = [df_v_conjug_mcq_1, df_v_conjug_mcq_2, df_v_conjug_mcq_3]\n",
    "df_v_conjug_mcq = pd.concat(frames_v_conjug_mcq)\n",
    "df_v_conjug_mcq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Verb Conjugation Cloze Test Exercises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_conjug_cloze_test(df_source, exo_id):\n",
    "    data = []\n",
    "    for i in range(len(df_source)):\n",
    "        doc_hi_phrase = nlp_hi_stanza(str(df_source[\"Hindi\"][i]))\n",
    "\n",
    "        count = 0\n",
    "        blank = []\n",
    "        answer = []\n",
    "        hint = []\n",
    "        for sent in doc_hi_phrase.sentences:\n",
    "            for word in sent.words:\n",
    "                if word.upos in [\"VERB\", \"AUX\"] and word.text != word.lemma:\n",
    "                    blank.append(count)\n",
    "                    answer.append(word.text)\n",
    "                    hint.append(word.lemma)\n",
    "\n",
    "                count += 1\n",
    "\n",
    "        if len(blank):\n",
    "            question = []\n",
    "\n",
    "            for b in range(len(blank)):\n",
    "                exo_id += 1\n",
    "                question.append(\n",
    "                    \" \".join([\n",
    "                        word.text \n",
    "                        if word.id != blank[b] + 1 \n",
    "                        else \"...\" \n",
    "                        for word in sent.words \n",
    "                        for sent in doc_hi_phrase.sentences\n",
    "                    ])\n",
    "                )\n",
    "                data.append([\n",
    "                    \"38\", \n",
    "                    \"Cloze_Test\", \n",
    "                    \"Grammar\", \n",
    "                    \"Verb_Conjugation\", \n",
    "                    str(exo_id), \n",
    "                    \"text\", \n",
    "                    \"text\", \n",
    "                    str(df_source[\"ID\"][i]), \n",
    "                    \"\", \n",
    "                    \"French\", \n",
    "                    \"Hindi\", \n",
    "                    str(df_source[\"Hindi\"][i]), \n",
    "                    \"Conjugate the verb correctly:\", \n",
    "                    f\"{question[b]} ({hint[b]})\", \n",
    "                    str(answer[b]), \n",
    "                    \"\", \n",
    "                    \"\", \n",
    "                    \"\", \n",
    "                    \"\", \n",
    "                    \"\", \n",
    "                    \"\",\n",
    "                ])\n",
    "\n",
    "    return exo_id, data\n",
    "\n",
    "exo_id = 0\n",
    "\n",
    "exo_id, data_v_conjug_ct_1 = verb_conjug_cloze_test(df_7000_sents_1, exo_id)\n",
    "df_v_conjug_ct_1 = pd.DataFrame(data_v_conjug_ct_1, columns=cols)\n",
    "\n",
    "exo_id, data_v_conjug_ct_2 = verb_conjug_cloze_test(df_7000_sents_2, exo_id)\n",
    "df_v_conjug_ct_2 = pd.DataFrame(data_v_conjug_ct_2, columns=cols)\n",
    "\n",
    "exo_id, data_v_conjug_ct_3 = verb_conjug_cloze_test(df_7000_sents_3, exo_id)\n",
    "df_v_conjug_ct_3 = pd.DataFrame(data_v_conjug_ct_3, columns=cols)\n",
    "\n",
    "frames_v_conjug_ct = [df_v_conjug_ct_1, df_v_conjug_ct_2, df_v_conjug_ct_3]\n",
    "df_v_conjug_ct = pd.concat(frames_v_conjug_ct)\n",
    "df_v_conjug_ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge All Exercise Dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_hi_exercises = [df_w_fc, df_spelling_mcq, df_s_fc, df_v_conjug_mcq, df_v_conjug_ct]\n",
    "df_hi_exercises = pd.concat(frames_hi_exercises)\n",
    "df_hi_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Exercise Dataframe to an Excel File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hi_exercises.to_excel(\"Hindi_Exercises.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('year_4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2dbc6a3e06803adbf086ce64bb7fe7554f6a053388168c89795f9a02e75d57e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
